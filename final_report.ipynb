{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Crown Counting - Final Report - SCC5830\n",
    "\n",
    "This is the last report concerning the experiments performed in order to count the tree crowns present in drone-captured data of a tropical forest.\n",
    "\n",
    "In section1, there is a description of the problem and the motivation behind it.  \n",
    "In section2, the input images are described in detail.  \n",
    "In section3, the first two failed approaches are presented, making an inventory of the lessons learned.  \n",
    "Section 4 presents the third and most successful experiment in great detail and future steps. Like the previous ones, the end of this section is a summary of lessons learned.  \n",
    "Section 5 is a brief conclusion.\n",
    "\n",
    "All work was done by me, Antonio Lutfi. This markdown file is generated from the jupyter notebook final_report.ipynb. It should be downloaded in order to run the code. Necessary libraries are listed under requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem at hand\n",
    "\n",
    "The main objective of this work is to segment drone-captured images of native forests into individual tree crowns. The main desired output is a simple counting of the trees, but any successful segmentation will also provide spacial data such as perimeter, area and altitude. This last one is possible because a 3d representation of the area is also available.\n",
    "\n",
    "The specific images for this experiment are of a green area within the UFRRJ campus. There is a single stitched image of the whole flight and the tests and processing will be done in segments, such as the one below:\n",
    "\n",
    "![](test.png)\n",
    "\n",
    "This application is intended to make forestry upkeep efforts less labor intensive. Today, making inventory of a forest requires frequent visits by foot. A reliable method to observe previously catalogued trees with a drone survey would make the job of forest engineers much easier.\n",
    "\n",
    "Images are provided by Professor [Bruno Mendonça](https://institutos.ufrrj.br/if/bruno-araujo-furtado-de-mendonca/) and Fernando Canto. They were obtained with support from FAPERJ and UFRRJ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Images\n",
    "\n",
    "Segmentation and tree counting will be done over a single image, generated by stitching together photos taken during a drone flight. The original stitched image is  24869 X 16367 pixels. A downscaled version is shown below:\n",
    "\n",
    "![](assets/whole_forest.png)\n",
    "\n",
    "Since this image was obtained by stitching, it was also possible to generate a 3D point cloud from the original drone footage, which helps tremendously with the task. [PyODM](https://github.com/OpenDroneMap/PyODM) was used to obtain such cloud. It is the python SDK for [OpenDroneMap](https://www.opendronemap.org/). This step is detailed at [pointcloud.ipynb](). Below, the point cloud and a depth-map, respectively.\n",
    "\n",
    "![](assets/pointcloud.png)\n",
    "![](assets/depthmap.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Crown Counting adapting the method proposed by Jing et al - 2012\n",
    "\n",
    "The third and final experiment consists of implementing a variation of the method proposed in \"An individual tree crown delineation method based on multi-scale segmentation of imagery\", available [here](https://www.sciencedirect.com/science/article/abs/pii/S0924271612000767).\n",
    "\n",
    "The modification is due to the visual aspects of the wooden area studied in the article and the one studied here. Even though Jing et al's forest is denser than many others found in different articles, it's still well behaved when compared to ours.\n",
    "\n",
    "![](assets/jingtree.png)\n",
    "\n",
    "Trees, while touching each other, have way clearer borders than in our case and are way more convex:\n",
    "\n",
    "![](test_img.png)\n",
    "\n",
    "Not only that, we have a point cloud available with the forest's depth information. For those reasons, the method had to be modified. I won't go into the original one, but only what I did differently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from PIL import Image, ImageEnhance\n",
    "from skimage.morphology import disk\n",
    "from img_utils import show, normalize\n",
    "from point_cloud_tools import process_point_cloud\n",
    "\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, G, B = 0, 1, 2\n",
    "H, W, CH = 0, 1, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image loading and downscale. The large resolution just slows things down while not adding much valuable information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaling = 8\n",
    "\n",
    "base_dir = '/media/antonio/Ubuntail/Dropbox/education/phd/courses/processamento_imagens/assignments/final'\n",
    "main_img_path = path.sep.join([base_dir, 'images', 'mataDS.tif'])\n",
    "\n",
    "p_cloud_dir = path.sep.join([base_dir, 'pyodm', 'entwine_pointcloud', 'ept-data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16367, 24869, 3)\n"
     ]
    }
   ],
   "source": [
    "main_img = cv2.imread(main_img_path)\n",
    "print(main_img.shape)\n",
    "main_img = cv2.resize(main_img, (main_img.shape[1] // downscaling, main_img.shape[0] // downscaling))\n",
    "resolution = main_img.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_map, _ = process_point_cloud(p_cloud_dir, resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results and images' resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(depth_map)\n",
    "print(depth_map.shape)\n",
    "show(main_img)\n",
    "print(main_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal is counting trees in the entire forest, but it's way faster to experiment with just a section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_head, h_tail = 600, 1100\n",
    "w_head, w_tail = 1000, 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = main_img[h_head:h_tail, w_head:w_tail].copy()\n",
    "depth = depth_map[h_head:h_tail, w_head:w_tail].copy()\n",
    "\n",
    "rgb = normalize(rgb, (0, 255), np.uint8)\n",
    "gray = normalize(cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY), (0, 255), np.uint8)\n",
    "depth = normalize(depth)\n",
    "\n",
    "show(rgb)\n",
    "show(gray)\n",
    "show(depth, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are some gaps in the depth map, a small dilation is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = cv2.dilate(depth, disk(3), iterations=1)\n",
    "show(depth, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "The method consists in five steps, and I'll explain them quoting the article directly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Scale analysys\n",
    "*To explore the sizes of tree crowns, a series of disk\n",
    "Structuring Elements (SEs) with a progressively increasing diameter from 3 in an increment of 2 pixels was employed in the (morphological) opening operation on the grayscale image [of the forest], and a series of\n",
    "opened images was generated. For any two consecutive opened images, Od and Od+2 , where subscripts indicate the corresponding SE diameters, their difference image (Od+2 - Od) was computed and its mean value was calculated. There are a number of local minima in these. A local minimum occurs when there are signiﬁcant differences in object sizes between two adjacent opened images. Therefore, these mean values reveal the dominant sizes of the objects and their size range captured in the image.*\n",
    "\n",
    "In this approach, however, I'll perform these successive morphological operations over the depth-map. This is due to previously stated reasons of unclear borders between trees and low convexity shapes.\n",
    "\n",
    "One other thing: I disagree --maybe due to a lack of understanding-- that the local minima in the means of differences mean a change of size. For me, this happens when an abrupt variation on these means occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that performs this morphological opening at different disk sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crown_size_layers(img, min_disk=3, max_disk=85):\n",
    "    opened = {'images': {}, 'mean_diffs': {}}\n",
    "    images = opened['images']\n",
    "    for disk_size in range(min_disk, max_disk, 2):\n",
    "        images[disk_size] = normalize(cv2.morphologyEx(img, cv2.MORPH_OPEN, disk(disk_size)))\n",
    "    \n",
    "    mean_diffs = opened['mean_diffs']\n",
    "    for i, disk_size in enumerate(images.keys()):\n",
    "        if disk_size == min_disk:\n",
    "            continue\n",
    "        diff = images[disk_size] - images[list(images.keys())[i - 1]]\n",
    "        euc_dist = np.sqrt(np.sum(diff ** 2))\n",
    "        mean_diffs[disk_size] = euc_dist\n",
    "    return opened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csl = get_crown_size_layers(depth)\n",
    "imgs = csl['images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the means of the difference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for sz in csl['mean_diffs']:\n",
    "    xs.append(sz)\n",
    "    ys.append(csl['mean_diffs'][sz])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,6)\n",
    "plt.plot(xs, ys, '-o')\n",
    "plt.xticks(np.arange(min(xs), max(xs) + 1, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing relevant sizes manually(for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crown_sizes = [19, 21, 23, 25, 27, 31, 33, 35, 37, 47, 49, 51, 57, 59, 61, 73, 77, 79, 81, 83]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Filtering\n",
    "\n",
    "*The multiple tree crown levels previously obtained using the scale analysis were used to design Gaussian ﬁlters for generating the multi-scale representation of the forest scene. Given any tree crown size of d pixels, a two-dimensional (2D) Gaussian ﬁlter with d * d window size was designed and its sigma (standard deviation, r) was set at a value of 0.3d pixels. This design was done in order to take account of various 3D radiometric shapes of tree crowns. When such a Gaussian ﬁlter was applied to a grayscale image, it was expected that tree crowns with a similar shape and size to the ﬁlter could be enhanced and smaller objects could be effectively suppressed.*\n",
    "\n",
    "Again, this expects a fairly round tree crown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered(img, crown_sizes):\n",
    "    filtered = {}\n",
    "    for sz in crown_sizes:\n",
    "        k = sz if sz % 2 == 1 else sz + 1\n",
    "        filtered[sz] = normalize(cv2.GaussianBlur(img, (k, k), 0.3 * k), (0, 255), np.uint8)\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = get_filtered(gray, crown_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sz in filtered:\n",
    "    show(filtered[sz])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Watershed Segmentation\n",
    "\n",
    "*Watershed segmentation applied for each gaussian-filtered version of the grayscale forest image obtained in step 2*\n",
    "\n",
    "The main difference from the article here is that we, again, take advantage of the depth map. Instead of choosing the seeds for the watershed based on local minima of the blurred images -as the article does- we get it from the depth map. The idea is that, for a given scale, we get the highest point within that scale's radius for each pixel, instead of the whitest. This is only for the seeds, though, as the segmentation itself will be performed on the filtered images form step 2.\n",
    "\n",
    "That means there will be small scales where branches will segmented, and large scales where maybe more than one tree crown will be clustered together. Further steps will refine these segmentations.\n",
    "\n",
    "There is a catch, though. In morphological opening, the disk radius is the same as of the generated artifacts. While in morphological dilation, which we use to find local maxima, the resulting radius doubles. So we have to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import segment_by_depth as segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_scale_segmenting(filtered, depth):\n",
    "    result = {}\n",
    "    depth = normalize(depth, (0, 255), np.uint8)\n",
    "    for sz in filtered:\n",
    "        se = disk(sz // 2)\n",
    "        segmented, debug = segment(normalize(filtered[sz], (0, 255), np.uint8),\n",
    "                                   depth,\n",
    "                                   structuring_el=se,\n",
    "                                   return_intermediate=True)\n",
    "        result[sz] = {'ws': segmented, 'debug': debug}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed = multi_scale_segmenting(filtered, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_debug(layers, filtered, orig_img, sz=None):\n",
    "    if sz is not None and sz.__class__ == int:\n",
    "        tf = lambda title: ' - '.join([str(sz), title])\n",
    "        layer = layers[sz]\n",
    "        debug = layer['debug']\n",
    "        show(debug['morph'], cmap='jet', title=tf('morph'))\n",
    "        show(debug['maxima'], title=tf('maxima'))\n",
    "        show(debug['maxima_open'], title=tf('maxima_open'))\n",
    "        show(debug['seeds'], cmap='jet', title=tf('seeds'))\n",
    "        edges = debug['edges']\n",
    "        \n",
    "        show(layer['ws'], cmap='jet', title=tf('ws'))\n",
    "        \n",
    "        canvas = filtered[sz].copy()\n",
    "        canvas[edges == 255] = 255\n",
    "        show(canvas, title=tf('overlay - filtered'))\n",
    "\n",
    "        canvas = orig_img.copy()\n",
    "        canvas[edges == 255] = (255, 255, 255)\n",
    "        show(canvas, title=tf('overlay - original'))\n",
    "    else:\n",
    "        szs = list(layers.keys()) if sz is None else sz\n",
    "        [view_debug(layers, filtered, orig_img, sz=s) for s in szs]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View process results at 4 different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_debug(watershed, filtered, rgb, [19, 33, 51, 79])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations regarding step 3\n",
    "\n",
    "It is clear that looking for local minima at the mean difference variation does not yield good segment sizes for this problem's domain. The larger sizes cluster a lot of trees together. It's necessary to experiment with other disk sizes and other methods of selecting the relevant ones. However, the segmentation itself usually follows the borders pretty well, so the method of dilating the depth map and segmenting over the filtered gray image is appropriate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pointcloud')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38dc056a69a1bc597bfaa95560c31600a73400e24db4efc9f675041f442fa471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
